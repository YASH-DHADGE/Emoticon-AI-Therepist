{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f2aa46a-f892-4f78-89c4-4b43b90ee35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Model will be saved to: D:\\Emoticon\\myapp\\backend\\EmpatheticChatbot-Kaggle-DialoGPT\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 1: Imports and Path Configuration ---\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "# --- CONFIGURE YOUR LOCAL PATHS HERE ---\n",
    "YOUR_SAVE_PATH = \"EmpatheticChatbot-Kaggle-DialoGPT\" \n",
    "BASE_MODEL = \"microsoft/DialoGPT-medium\"\n",
    "os.makedirs(YOUR_SAVE_PATH, exist_ok=True)\n",
    "print(f\"Setup complete. Model will be saved to: {os.path.abspath(YOUR_SAVE_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "556a64c4-1f8e-4edd-9efa-f8e5f4317ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up tokenizer...\n",
      "Tokenizer is ready.\n",
      "‚ö†Ô∏è No GPU found. Using CPU. Training will be VERY slow.\n"
     ]
    }
   ],
   "source": [
    "# --- CELL 2: Tokenizer and Hardware Check ---\n",
    "print(\"Setting up tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "EOS = tokenizer.eos_token\n",
    "print(\"Tokenizer is ready.\")\n",
    "\n",
    "# Check for GPU and set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"‚úÖ GPU found! Using device: {device}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"‚ö†Ô∏è No GPU found. Using CPU. Training will be VERY slow.\")\n",
    "\n",
    "use_fp16 = True if device == \"cuda\" else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b879afe-0383-4630-800b-4dc235981a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE REAL COLUMN NAMES ARE:\n",
      "['Unnamed: 0', 'Situation', 'emotion', 'empathetic_dialogues', 'labels', 'Unnamed: 5', 'Unnamed: 6']\n"
     ]
    }
   ],
   "source": [
    "# --- DIAGNOSTIC CELL ---\n",
    "# Run this cell by itself.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "kaggle_csv_path = \"empathetic_dialogues.csv\"\n",
    "df_empathetic = pd.read_csv(kaggle_csv_path)\n",
    "\n",
    "print(\"THE REAL COLUMN NAMES ARE:\")\n",
    "print(df_empathetic.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f633be31-ad12-4195-baa3-c5ab02ee68f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and processing datasets...\n",
      "Reading Kaggle dataset from: empathetic_dialogues.csv\n",
      "Loaded 64636 dialogues from CSV.\n",
      "\n",
      "Loading 'blended_skill_talk' dataset from Hugging Face...\n",
      "--- Raw Datasets Loaded Successfully ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d26d98844279439487cdef15909ff375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5309cb5e03744214b7e3480d61b4162f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191d2c21f0a14a38bca00198cf6a11fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/980 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Datasets Combined. Train examples: 62991, Eval examples: 7473 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ed151198b347308cf272c71ad193ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/62991 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c3975288484a3f919f6bc8d085c4ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Datasets are tokenized and ready for training.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load and Process Datasets (FINAL WORKING VERSION)\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "print(\"Loading and processing datasets...\")\n",
    "\n",
    "# --- 1. LOAD THE KAGGLE CSV ---\n",
    "kaggle_csv_path = \"empathetic_dialogues.csv\"\n",
    "if not os.path.exists(kaggle_csv_path):\n",
    "    raise FileNotFoundError(f\"ERROR: File not found at '{os.path.abspath(kaggle_csv_path)}'.\")\n",
    "\n",
    "print(f\"Reading Kaggle dataset from: {kaggle_csv_path}\")\n",
    "df_empathetic = pd.read_csv(kaggle_csv_path)\n",
    "print(f\"Loaded {len(df_empathetic)} dialogues from CSV.\")\n",
    "\n",
    "\n",
    "# --- 2. LOAD PERSONA-CHAT ---\n",
    "print(\"\\nLoading 'blended_skill_talk' dataset from Hugging Face...\")\n",
    "persona_dataset = load_dataset(\"blended_skill_talk\")\n",
    "print(\"--- Raw Datasets Loaded Successfully ---\")\n",
    "\n",
    "\n",
    "# --- 3. DEFINE FORMATTING FUNCTIONS (WITH ALL CORRECTIONS) ---\n",
    "def format_kaggle_dialogue(row):\n",
    "    \"\"\"Uses the correct column names for the Kaggle CSV.\"\"\"\n",
    "    prompt = str(row['Situation']).strip()\n",
    "    response = str(row['empathetic_dialogues']).strip()\n",
    "    return f\"{prompt}{EOS}{response}{EOS}\"\n",
    "\n",
    "\n",
    "def format_persona_dialogue(example):\n",
    "    \"\"\"\n",
    "    Uses the correct column names for the blended_skill_talk dataset.\n",
    "    The key is 'personas', not 'context_personas'.\n",
    "    \"\"\"\n",
    "    # --- THIS IS THE LINE THAT HAS BEEN FIXED ---\n",
    "    persona = \" \".join(example[\"personas\"])\n",
    "    # -------------------------------------------\n",
    "    \n",
    "    context = example[\"previous_utterance\"]\n",
    "    full_text = persona + EOS + \" \".join(context) + EOS + example[\"free_messages\"][0]\n",
    "    return {\"text\": full_text + EOS}\n",
    "\n",
    "\n",
    "# --- 4. APPLY FORMATTING ---\n",
    "df_empathetic['text'] = df_empathetic.apply(format_kaggle_dialogue, axis=1)\n",
    "empathetic_formatted = Dataset.from_pandas(df_empathetic)\n",
    "\n",
    "# This line will now work correctly.\n",
    "persona_formatted = persona_dataset.map(\n",
    "    format_persona_dialogue,\n",
    "    remove_columns=list(persona_dataset['train'].features)\n",
    ")\n",
    "\n",
    "\n",
    "# --- 5. SPLIT AND CONCATENATE DATASETS ---\n",
    "empathetic_train = empathetic_formatted.train_test_split(test_size=0.1, seed=42)\n",
    "persona_train = persona_formatted['train']\n",
    "persona_eval = persona_formatted['validation']\n",
    "train_dataset = concatenate_datasets([empathetic_train['train'], persona_train]).shuffle(seed=42)\n",
    "eval_dataset = concatenate_datasets([empathetic_train['test'], persona_eval])\n",
    "print(f\"\\n--- Datasets Combined. Train examples: {len(train_dataset)}, Eval examples: {len(eval_dataset)} ---\")\n",
    "\n",
    "\n",
    "# --- 6. TOKENIZE ---\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True, remove_columns=eval_dataset.column_names)\n",
    "\n",
    "print(\"‚úÖ Datasets are tokenized and ready for training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bb96742-7be1-4979-8ed1-0f07b148d662",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Maybe you meant '==' or ':=' instead of '='? (807134386.py, line 18)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31moutput_dir=\"D:\\Emoticon\\myapp\\backend\\EmoticonAi\",       # The directory where checkpoints will be saved.\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Maybe you meant '==' or ':=' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Configure and Run Training\n",
    "\n",
    "print(\"Configuring and starting the training process...\")\n",
    "\n",
    "# --- 1. LOAD THE BASE MODEL ---\n",
    "# This loads the pre-trained DialoGPT model from Hugging Face.\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL)\n",
    "\n",
    "# --- 2. DEFINE THE DATA COLLATOR ---\n",
    "# The data collator is a utility that will batch our processed examples together.\n",
    "# For causal language modeling (like GPT), we don't need a special mask (mlm=False).\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# --- 3. DEFINE TRAINING ARGUMENTS ---\n",
    "# This object contains all the parameters to configure the training process.\n",
    "use_fp16 = True  # or False, depending on your setup\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=r\"D:\\Emoticon\\myapp\\backend\\EmoticonAi\",  # or use double backslashes or forward slashes\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "\n",
    "    # Evaluation and Saving Strategy\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    # Performance and Logging\n",
    "    fp16=use_fp16,\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "\n",
    "# --- 4. INITIALIZE THE TRAINER ---\n",
    "# The Trainer object orchestrates the entire training and evaluation process.\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# --- 5. START TRAINING ---\n",
    "print(\"üöÄ Starting fine-tuning... This can take several hours depending on your hardware.\")\n",
    "print(\"   You will see loss metrics and progress bars below.\")\n",
    "trainer.train()\n",
    "print(\"‚úÖ Fine-tuning complete!\")\n",
    "\n",
    "\n",
    "# --- 6. SAVE THE FINAL BEST MODEL ---\n",
    "# The trainer automatically loaded the best model, so we just need to save it.\n",
    "print(\"\\nSaving final best model to a clean directory...\")\n",
    "final_model_path = os.path.join(D:\\Emoticon\\myapp\\backend\\EmoticonAi, \"final_best_model\")\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path) # Important: also save the tokenizer with the model\n",
    "\n",
    "print(f\"‚úÖ All steps complete! Your best model is saved at: {os.path.abspath(final_model_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1d42a75-62ef-4b1c-8f97-78212fc07dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ERROR: Model not found at 'D:\\Emoticon\\myapackend\\EmpatheticChatbot-Kaggle-DialoGPT\\final_best_model'\n",
      "Please make sure you have run the training script successfully and the path is correct.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Interactive Test Case\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# --- 1. LOAD THE FINE-TUNED MODEL ---\n",
    "# This path must point to the folder where the best model was saved.\n",
    "# It should contain files like `pytorch_model.bin` and `config.json`.\n",
    "final_model_path = os.path.join(\"D:\\Emoticon\\myapp\\backend\\EmpatheticChatbot-Kaggle-DialoGPT\", \"final_best_model\")\n",
    "\n",
    "if not os.path.exists(final_model_path):\n",
    "    print(\"=\"*50)\n",
    "    print(f\"ERROR: Model not found at '{os.path.abspath(final_model_path)}'\")\n",
    "    print(\"Please make sure you have run the training script successfully and the path is correct.\")\n",
    "    print(\"=\"*50)\n",
    "else:\n",
    "    print(f\"Loading model from: {os.path.abspath(final_model_path)}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(final_model_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(final_model_path)\n",
    "\n",
    "    # Check for GPU and move model to the correct device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "    print(f\"Model loaded on {device.upper()}.\")\n",
    "\n",
    "\n",
    "    # --- 2. SETUP THE INTERACTIVE CHAT LOOP ---\n",
    "    # This tensor will store the entire conversation history\n",
    "    chat_history_ids = None\n",
    "\n",
    "    print(\"\\nStarting Chatbot. Type 'quit' to exit.\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_input = input(\">> You: \")\n",
    "\n",
    "            if user_input.lower() == 'quit':\n",
    "                print(\"Bot: Goodbye!\")\n",
    "                break\n",
    "\n",
    "            # Encode the new user input, add the eos_token, and return a tensor in PyTorch\n",
    "            new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt').to(device)\n",
    "\n",
    "            # Append the new user input tokens to the chat history\n",
    "            # If this is the first turn, bot_input_ids is just the user input.\n",
    "            # Otherwise, it's the concatenation of the past history and the new input.\n",
    "            bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if chat_history_ids is not None else new_user_input_ids\n",
    "\n",
    "            # --- 3. GENERATE A RESPONSE ---\n",
    "            # The model.generate() function creates the response.\n",
    "            # The `chat_history_ids` now becomes the full conversation history.\n",
    "            chat_history_ids = model.generate(\n",
    "                bot_input_ids,\n",
    "                max_length=1000,                               # Max length of the entire conversation\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                no_repeat_ngram_size=3,                       # Prevents the model from repeating the same 3-word phrases\n",
    "                do_sample=True,                               # Activates sampling-based generation (more creative)\n",
    "                top_k=50,                                     # Considers the top 50 most likely words at each step\n",
    "                top_p=0.95,                                   # Nucleus sampling: keeps the most probable tokens with cumulative probability >= 0.95\n",
    "                temperature=0.7                               # Makes the output less random (lower) or more random (higher)\n",
    "            )\n",
    "\n",
    "            # --- 4. DECODE AND PRINT THE RESPONSE ---\n",
    "            # Decode the last response from the bot.\n",
    "            # `bot_input_ids.shape[-1]:` ensures we only decode the new part of the history.\n",
    "            response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "            print(f\"Bot: {response}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ab6593-75d4-4a39-bada-b43d521442f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
